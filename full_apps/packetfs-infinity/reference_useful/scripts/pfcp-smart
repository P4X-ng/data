#!/usr/bin/env python3
"""
pfcp-smart - Smart PacketFS transfer over SSH with auto-bootstrap

Automatically detects if PacketFS is installed on remote.
If not, temporarily installs a minimal decoder!

Usage:
    pfcp-smart file.txt user@server:/path/
    pfcp-smart bigfile.bin server:~/
    pfcp-smart -r folder/ server:/backup/
"""

import sys
import os
import subprocess
import tempfile
import hashlib
import zlib
import json
import time
import base64
from pathlib import Path

# Minimal PFS decoder that we'll ship to remote if needed
MINIMAL_DECODER = '''
#!/usr/bin/env python3
import sys, json, zlib, hashlib, os

def decompress_pfs(pfs_file, output_dir):
    """Minimal PFS decompressor."""
    with open(pfs_file, "rb") as f:
        magic = f.read(4)
        if magic != b"PFS1":
            print("Error: Invalid PFS file", file=sys.stderr)
            return False
        
        meta_len = int.from_bytes(f.read(4), "big")
        metadata = json.loads(f.read(meta_len))
        
        # Check for blob reference (advanced mode)
        if metadata.get("blob_ref"):
            # Use shared blob for reconstruction
            blob_path = f"/tmp/.pfs_blob_{metadata['blob_id']}"
            if os.path.exists(blob_path):
                with open(blob_path, 'rb') as blob:
                    # Reconstruct from offsets
                    data = bytearray()
                    for offset, length in metadata['chunks']:
                        blob.seek(offset)
                        data.extend(blob.read(length))
            else:
                # Fallback to compressed data
                data = zlib.decompress(f.read())
        else:
            # Simple compression mode
            data = zlib.decompress(f.read())
        
        # Verify integrity
        if hashlib.sha256(data).hexdigest() != metadata['sha256']:
            print("Error: Integrity check failed", file=sys.stderr)
            return False
        
        # Write output
        output_path = os.path.join(output_dir, metadata['filename'])
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'wb') as out:
            out.write(data)
        
        print(f"OK:{output_path}")
        return True

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: pfs_decode <pfs_file> <output_dir>", file=sys.stderr)
        sys.exit(1)
    
    success = decompress_pfs(sys.argv[1], sys.argv[2])
    sys.exit(0 if success else 1)
'''

class SmartPFSTransfer:
    def __init__(self, ssh_host, remote_path):
        self.ssh_host = ssh_host
        self.remote_path = remote_path
        self.remote_has_pfs = None
        self.remote_pfs_version = None
        self.temp_decoder_path = None
        self.blob_cache = {}
        
    def check_remote_pfs(self):
        """Check if PacketFS is installed on remote."""
        print("üîç Checking remote for PacketFS...", end='', flush=True)
        
        # Try to run pfcp --version on remote
        result = subprocess.run(
            ['ssh', self.ssh_host, 'which pfcp 2>/dev/null && pfcp --version 2>/dev/null || echo "NOT_FOUND"'],
            capture_output=True,
            text=True
        )
        
        if "NOT_FOUND" in result.stdout:
            print(" not found")
            self.remote_has_pfs = False
            return False
        else:
            print(" ‚úÖ found!")
            self.remote_has_pfs = True
            self.remote_pfs_version = result.stdout.strip()
            return True
    
    def install_temp_decoder(self):
        """Install temporary PFS decoder on remote."""
        print("üì¶ Installing temporary PFS decoder...", end='', flush=True)
        
        # Create temp decoder path
        self.temp_decoder_path = f"/tmp/pfs_decoder_{os.getpid()}.py"
        
        # Write decoder to remote
        result = subprocess.run(
            ['ssh', self.ssh_host, f'cat > {self.temp_decoder_path} && chmod +x {self.temp_decoder_path}'],
            input=MINIMAL_DECODER.encode(),
            capture_output=True
        )
        
        if result.returncode != 0:
            print(" ‚ùå failed")
            return False
        
        print(" ‚úÖ")
        return True
    
    def create_blob_chunk(self, data):
        """Create reusable blob chunks using content-defined chunking."""
        # Simple chunking for demo - in production use rolling hash
        CHUNK_SIZE = 4096
        chunks = []
        
        for i in range(0, len(data), CHUNK_SIZE):
            chunk = data[i:i+CHUNK_SIZE]
            chunk_hash = hashlib.sha256(chunk).hexdigest()[:16]
            
            if chunk_hash not in self.blob_cache:
                self.blob_cache[chunk_hash] = chunk
            
            chunks.append((chunk_hash, len(chunk)))
        
        return chunks
    
    def transfer_file(self, local_file):
        """Transfer file using smart PFS protocol."""
        
        # Read file
        with open(local_file, 'rb') as f:
            data = f.read()
        
        file_size = len(data)
        file_name = Path(local_file).name
        
        print(f"\nüìä Processing {file_name} ({file_size/1024:.1f} KB)...")
        
        # Create chunks or compress
        use_blob = file_size > 10240  # Use blob for files > 10KB
        
        if use_blob and self.remote_has_pfs:
            # Advanced mode: use blob chunks
            print("üß© Creating content-defined chunks...", end='', flush=True)
            chunks = self.create_blob_chunk(data)
            print(f" {len(chunks)} chunks")
            
            # Send only new chunks
            new_chunks = self.send_blob_chunks(chunks)
            
            metadata = {
                'filename': file_name,
                'original_size': file_size,
                'sha256': hashlib.sha256(data).hexdigest(),
                'blob_ref': True,
                'chunks': [(c[0], c[1]) for c in chunks],
                'version': 2
            }
            
            # Send only references
            compressed = b''  # Empty - we're using blob!
            
        else:
            # Simple mode: just compress
            print("üóúÔ∏è Compressing...", end='', flush=True)
            compressed = zlib.compress(data, level=9)
            compression_ratio = (1 - len(compressed) / file_size) * 100
            print(f" {compression_ratio:.1f}% smaller")
            
            metadata = {
                'filename': file_name,
                'original_size': file_size,
                'compressed_size': len(compressed),
                'sha256': hashlib.sha256(data).hexdigest(),
                'compression': 'zlib9',
                'version': 1
            }
        
        # Create PFS package
        with tempfile.NamedTemporaryFile(suffix='.pfs', delete=False) as tmp:
            tmp.write(b'PFS1')
            meta_json = json.dumps(metadata).encode()
            tmp.write(len(meta_json).to_bytes(4, 'big'))
            tmp.write(meta_json)
            tmp.write(compressed)
            tmp_path = tmp.name
        
        try:
            # Transfer the PFS file
            remote_pfs = f"/tmp/{file_name}.pfs.{os.getpid()}"
            
            transfer_size = os.path.getsize(tmp_path)
            print(f"üì° Transferring {transfer_size/1024:.1f} KB (was {file_size/1024:.1f} KB)...")
            
            result = subprocess.run(
                ['scp', '-q', tmp_path, f"{self.ssh_host}:{remote_pfs}"],
                capture_output=True
            )
            
            if result.returncode != 0:
                print(f"‚ùå Transfer failed: {result.stderr.decode()}")
                return False
            
            # Decompress on remote
            print("üîì Decompressing on remote...", end='', flush=True)
            
            if self.remote_has_pfs:
                # Use native PFS
                cmd = f"pfcp-decode {remote_pfs} {self.remote_path}"
            else:
                # Use temporary decoder
                cmd = f"python3 {self.temp_decoder_path} {remote_pfs} {self.remote_path}"
            
            result = subprocess.run(
                ['ssh', self.ssh_host, cmd],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                # Fallback to simple inline decompression
                decompress_script = f'''
import sys, json, zlib, os
with open("{remote_pfs}", "rb") as f:
    magic = f.read(4)
    if magic != b"PFS1": sys.exit(1)
    meta_len = int.from_bytes(f.read(4), "big")
    metadata = json.loads(f.read(meta_len))
    compressed = f.read()
    if compressed:
        data = zlib.decompress(compressed)
    else:
        print("Blob mode not supported in fallback")
        sys.exit(1)
    output = os.path.join("{self.remote_path}", metadata["filename"])
    os.makedirs(os.path.dirname(output), exist_ok=True)
    with open(output, "wb") as out:
        out.write(data)
os.remove("{remote_pfs}")
print("OK")
'''
                result = subprocess.run(
                    ['ssh', self.ssh_host, 'python3', '-c', decompress_script],
                    capture_output=True,
                    text=True
                )
            
            if "OK" in result.stdout:
                print(" ‚úÖ Success!")
            else:
                print(" ‚ùå Failed")
                return False
            
            # Show stats
            speedup = file_size / transfer_size
            print(f"\n‚ú® PacketFS Magic:")
            print(f"   ‚Ä¢ Original: {file_size/1024:.1f} KB") 
            print(f"   ‚Ä¢ Transferred: {transfer_size/1024:.1f} KB")
            print(f"   ‚Ä¢ Speedup: {speedup:.1f}x effective transfer")
            if use_blob and self.remote_has_pfs:
                print(f"   ‚Ä¢ Used content-defined chunking (blob mode)")
            
            return True
            
        finally:
            os.unlink(tmp_path)
    
    def send_blob_chunks(self, chunks):
        """Send new blob chunks to remote."""
        # In production: check which chunks remote already has
        # For now: simplified version
        new_chunks = []
        
        for chunk_hash, chunk_size in chunks:
            # Check if remote has this chunk (simplified)
            # In production: batch check all chunks
            new_chunks.append((chunk_hash, chunk_size))
        
        return new_chunks
    
    def cleanup(self):
        """Clean up temporary files on remote."""
        if self.temp_decoder_path:
            print("üßπ Cleaning up temporary decoder...", end='', flush=True)
            subprocess.run(
                ['ssh', self.ssh_host, f'rm -f {self.temp_decoder_path}'],
                capture_output=True
            )
            print(" ‚úÖ")

def main():
    if len(sys.argv) < 3:
        print(__doc__)
        sys.exit(1)
    
    source = sys.argv[1]
    dest = sys.argv[2]
    
    if not Path(source).exists():
        print(f"Error: {source} not found", file=sys.stderr)
        sys.exit(1)
    
    # Parse destination
    if ':' not in dest:
        print("Error: Destination must be user@host:/path or host:/path", file=sys.stderr)
        sys.exit(1)
    
    ssh_host, remote_path = dest.rsplit(':', 1)
    if not remote_path:
        remote_path = '~/'
    
    # Create smart transfer object
    transfer = SmartPFSTransfer(ssh_host, remote_path)
    
    try:
        # Check if remote has PFS
        if not transfer.check_remote_pfs():
            # Install temporary decoder
            if not transfer.install_temp_decoder():
                print("Error: Could not install temporary decoder", file=sys.stderr)
                sys.exit(1)
        
        # Transfer the file
        success = transfer.transfer_file(source)
        
        if not success:
            sys.exit(1)
            
    finally:
        # Always cleanup
        transfer.cleanup()

if __name__ == '__main__':
    main()