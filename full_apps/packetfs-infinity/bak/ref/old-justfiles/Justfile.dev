
# GPU coordinator (host endpoint)
# Example:
#   just dev-gpu-coordinator path=/dev/shm/pfs_blob.bin winmb=1 devices=0,1 conc=8 max=0
#   (set max>0 for a limited run)

# GPU coordinator (env-driven)
# Usage: FILE=/dev/shm/pfs_blob.bin ENDPOINT=https://127.0.0.1:{{WS_PORT}} WINMB=1 DEVICES=0 CONCURRENCY=8 MAX=0 PROGRAM='[...]' just dev gpu-coordinator
# Direct: just dev-gpu-coordinator

dev-gpu-coordinator:
    bash scripts/run/pfs_swarm_coordinator_env.sh


# Help for this file (when used standalone)
help-dev:
    @echo "dev commands: up|install|test|clean|spider-assets|vpcpu-init|vpcpu-add|vpcpu-list|vpcpu-compute|vcpu-service-up|vcpu-service-down|vcpu-smoke|data-blob|bench-local|bench-edge|gh-build-manifests|gh-fetch-index|gh-eval|gh-export-repo|pktop|milkshake|milkshake-aws-setup|milkshake-aws-query|milkshake-aws-word"

# Convenience pinned variant
# usage: just dev-gpu-smoke-pinned path=/dev/shm/pfs_blob.bin imm=0 cpu=0
dev-gpu-smoke-pinned path="/dev/shm/pfs_blob.bin" imm="0" cpu="0" offset="" length="" endpoint="http://127.0.0.1:{{WS_PORT}}":
    @echo "[gpu] pinned to CPU {{cpu}}"
    PFS_PIN_CPU={{cpu}} just dev-gpu-smoke path="{{path}}" imm="{{imm}}" offset="{{offset}}" length="{{length}}" endpoint="{{endpoint}}"


# GPU smoke test against /gpu/compute (env-driven)
# Usage: FILE=/dev/shm/pfs_blob.bin ENDPOINT=https://127.0.0.1:{{WS_PORT}} IMM=0 just dev gpu-smoke
# Optional: OFFSET, LENGTH
# Direct: just dev-gpu-smoke

dev-gpu-smoke:
    WS_PORT={{WS_PORT}} bash scripts/gpu/run_compute_smoke.sh

# GPU program smoke against /gpu/program (uses HTTPS with -k)
# Usage (env-based): FILE=/etc/hosts ENDPOINT=https://127.0.0.1:{{WS_PORT}} PROGRAM='[{"op":"xor","imm":255},{"op":"counteq","imm":0}]' just dev gpu-program-smoke
# Optional: OFFSET=0 LENGTH=4096
# Falls back to WS_PORT from justfile.vars when ENDPOINT is unset
# Note: Use FILE (not PATH) to avoid clobbering PATH env
# Dispatch: just dev gpu-program-smoke
# Direct:   just dev-gpu-program-smoke

dev-gpu-program-smoke:
    FILE="${FILE:-}" ENDPOINT="${ENDPOINT:-}" OFFSET="${OFFSET:-}" LENGTH="${LENGTH:-}" PROGRAM="${PROGRAM:-}" WS_PORT={{WS_PORT}} bash scripts/gpu/run_program_smoke.sh

# Dev and local workflows (core-only)

# Include central variables
import "./justfile.vars"

# Start dev server (no Redis, no swarm)
dev-up:
    @echo "Starting pfs-infinity dev server on http://0.0.0.0:{{WS_PORT}}"
    PYTHONPATH=. {{VENV_PATH}}/bin/hypercorn -b 0.0.0.0:{{WS_PORT}} app.main:app --reload

# Back-compat default alias
up:
    just dev up

# Dev: install
# Uses central venv as per repo rules
dev-install:
    {{VENV_PATH}}/bin/python -m pip install -U pip setuptools wheel
    {{VENV_PATH}}/bin/python -m pip install fastapi hypercorn pydantic python-multipart pytest aioquic aiortc httpx aiohttp websockets cryptography
    @echo "[dev] Installing pfcp command..."
    just transfer install-pfcp

# Tests (prefix per rules)
dev-test:
    PYTHONPATH=. {{VENV_PATH}}/bin/python -m pytest -q tests/test_health.py tests/test_twin_basic.py

# Clean artifacts (build, caches, logs)
dev-clean:
    bash scripts/clean.sh

# Dispatcher: `just dev <cmd>` (core)
dev cmd="help":
    bash scripts/dispatch/dev_dispatch.sh "{{cmd}}"

# Asset spider: discovers range/cache-capable data URLs and records them
# Example:
#   just dev spider-assets seeds=https://releases.ubuntu.com/ allow=releases.ubuntu.com range=1
#   just dev spider-assets seeds=https://releases.ubuntu.com/ allow=releases.ubuntu.com range=1 compute_kind=edge_http compute_endpoint=http://localhost:5000

dev-spider-assets seeds allow range="0" compute_kind="" compute_endpoint="" max_pages="50":
    @echo "[spider] seeds={{seeds}} allow={{allow}} range={{range}} compute_kind={{compute_kind}}"
    bash scripts/spider/run_spider.sh "{{seeds}}" "{{allow}}" "{{max_pages}}" "{{range}}" "{{compute_kind}}" "{{compute_endpoint}}"

# Virtual pCPU registry and runner

dev-vpcpu-init:
    @mkdir -p var/vpcpu
    PYTHONPATH=. {{VENV_PATH}}/bin/python scripts/vpcpu/init_db.py

# Example:
# just dev vpcpu-add name=cf-worker kind=worker endpoint=https://pfs-edge.username.workers.dev attrs='{"latency_ms":25,"region":"SJC","concurrency":16}'

dev-vpcpu-add name kind endpoint attrs='{}':
    PYTHONPATH=. {{VENV_PATH}}/bin/python scripts/vpcpu/register_asset.py --name {{name}} --kind {{kind}} --endpoint {{endpoint}} --attrs '{{attrs}}'

# List assets

dev-vpcpu-list:
    PYTHONPATH=. {{VENV_PATH}}/bin/python scripts/vpcpu/list_assets.py

# Run a compute job via a provider endpoint
# Example: just dev vpcpu-compute kind=worker endpoint=https://pfs-edge... data_url=https://... instructions='[{"op":"counteq","imm":69,"offset":0,"length":1048576}]'

dev-vpcpu-compute kind endpoint data_url instructions:
    {{VENV_PATH}}/bin/python scripts/vpcpu/run_edge_compute_job.py --kind {{kind}} --endpoint {{endpoint}} --data-url {{data_url}} --instructions '{{instructions}}'

# Fast local blob generator (tmpfs)
# Example: just dev data-blob path=/dev/shm/pfs_blob.bin size_mb=256 pattern=zero

dev-data-blob path size_mb pattern="zero":
    bash scripts/data/make_blob.sh {{path}} {{size_mb}} {{pattern}}

# vCPU service (host)
# Example: just dev vcpu-service-up blob=/dev/shm/pfs_blob.bin port=9855
# Metrics written to logs/pfs_vcpu_stats.jsonl
dev-vcpu-service-up host="127.0.0.1" port="9855" blob="/dev/shm/pfs_blob.bin" metrics="logs/pfs_vcpu_stats.jsonl":
    @echo "[vcpu-service] host={{host}} port={{port}} blob={{blob}} metrics={{metrics}}"
    {{VENV_PATH}}/bin/python scripts/vcpu/pfs_vcpu_service.py --host {{host}} --port {{port}} --blob {{blob}} --metrics {{metrics}}

# Stop hint (service is foreground; ctrl-c)
dev-vcpu-service-down:
    @echo "[vcpu-service] Run in foreground; ctrl-c to stop"

# vCPU smoke client (guest/container)
# Example: just dev vcpu-smoke host=127.0.0.1 port=9855 blob_bytes=1048576 win=4096 op=counteq imm=0
dev-vcpu-smoke host="127.0.0.1" port="9855" blob_bytes="1048576" win="4096" op="counteq" imm="0" align="64":
    {{VENV_PATH}}/bin/python scripts/vcpu/vcpu_client.py --host {{host}} --port {{port}} --blob-bytes {{blob_bytes}} --win {{win}} --op {{op}} --imm {{imm}} --align {{align}}

# Local streaming bench against /compute on file:// path
# Example: just dev bench-local path=/dev/shm/pfs_blob.bin conc=8 wins=256 op=counteq imm=0

dev-bench-local path conc="8" wins="256" op="counteq" imm="0" endpoint="http://localhost:{{WS_PORT}}" winmb="1":
    {{VENV_PATH}}/bin/python scripts/bench/bench_local_stream.py --endpoint {{endpoint}} --path {{path}} --window-mb {{winmb}} --concurrency {{conc}} --max-windows {{wins}} --op {{op}} --imm {{imm}}

# Build GitHub Pages manifests (POC)
# Example:
#   just dev-gh-build-manifests path=/dev/shm/pfs_blob.bin base_url=https://USER.github.io/pfs-index/data/sha256/<digest>.bin

dev-gh-build-manifests path base_url provider_id="pages" window_mb="1" outdir="docs":
    PYTHONPATH=. {{VENV_PATH}}/bin/python scripts/publish/gh_build_manifests.py --object-path {{path}} --base-url {{base_url}} --provider-id {{provider_id}} --window-mb {{window_mb}} --output-dir {{outdir}}

# Fetch manifests from Pages and build a local window index
# Example:
#   just dev-gh-fetch-index manifest_url=https://USER.github.io/pfs-index/super_manifest.json out=var/vpcpu/index/pages_index.json

dev-gh-fetch-index manifest_url out="var/vpcpu/index/pages_index.json":
    PYTHONPATH=. {{VENV_PATH}}/bin/python scripts/publish/gh_fetch_index.py --manifest-url {{manifest_url}} --out {{out}}

# Evaluate a manifest by streaming windows via /compute
# Example:
#   just dev-gh-eval manifest_url=https://USER.github.io/pfs-index/super_manifest.json endpoint=http://localhost:5000 op=counteq imm=0


# Positional alternative: just dev-gh-eval https://...  http://localhost:5000 counteq 0

dev-gh-eval manifest_url endpoint op="counteq" imm="0" provider_id="pages" max="64" batch="16" conc="4":
    PYTHONPATH=. {{VENV_PATH}}/bin/python scripts/run/pfs_eval_manifest.py --manifest-url {{manifest_url}} --endpoint {{endpoint}} --op {{op}} --imm {{imm}} --provider-id {{provider_id}} --max-windows {{max}} --batch {{batch}} --concurrency {{conc}}

# Export docs/ into a private data repo working directory
# Example:
#   just dev-gh-export-repo src=docs dst=../pfs-index branch=main

dev-gh-export-repo src="docs" dst="../pfs-index" branch="main":
    bash scripts/publish/gh_export_repo.sh {{src}} {{dst}} {{branch}}

# Duration-based edge bench (writes var/vpcpu/baselines.jsonl)
# Example (positional): just dev-bench-edge /dev/shm/pfs_blob.bin 8 15 counteq 0

dev-bench-edge path conc="8" seconds="60" op="counteq" imm="0" endpoint="http://localhost:5000" winmb="1" label="local":
    {{VENV_PATH}}/bin/python scripts/bench/pfs_bench_edge.py --endpoint {{endpoint}} --path {{path}} --window-mb {{winmb}} --concurrency {{conc}} --duration-s {{seconds}} --op {{op}} --imm {{imm}} --label {{label}}

# Packet monitor (pfs-pktop)
# Example: just dev pktop interval=0.5 top=15
# Installs a symlink to central venv for convenience as 'pfs-pktop'
dev-pktop interval="1.0" top="10" include_lo="0" metrics="" iface="":
    @echo "[pktop] interval={{interval}} top={{top}} include_lo={{include_lo}} metrics={{metrics}} iface={{iface}}"
    @ln -sf $(pwd)/scripts/monitor/pfs_pktop.py {{VENV_PATH}}/bin/pfs-pktop 2>/dev/null || true
    {{VENV_PATH}}/bin/python scripts/monitor/pfs_pktop.py --interval {{interval}} --top {{top}} $([ "{{include_lo}}" = "1" ] && echo --include-lo || true) $([ -n "{{metrics}}" ] && echo --metrics "{{metrics}}" || true) $([ -n "{{iface}}" ] && echo --iface "{{iface}}" || true)

# Multi-port word program (cloud)
# Example:
#   just dev milkshake-aws-word target_ip=203.0.113.10 base=52000 bits=16 pattern=0b1011001110001111 log_group=/aws/vpc/flow-logs/my-vpc eni=eni-abc region=us-east-1 seconds=2 pps=2000

dev-milkshake-aws-word target_ip base pattern log_group eni bits="16" pps="2000" seconds="2" minutes="5" region="" prefer_action="ACCEPT" threshold="1":
    @echo "[milkshake:aws] word program to {{target_ip}} base={{base}} bits={{bits}} pattern={{pattern}} region={{region}}"
    bash scripts/cloud_milkshake/run_word.sh "{{target_ip}}" "{{base}}" "{{bits}}" "{{pattern}}" "{{pps}}" "{{seconds}}" "{{log_group}}" "{{eni}}" "{{minutes}}" "{{region}}" "{{prefer_action}}" "{{threshold}}"

# Milkshake (cloud, AWS VPC Flow Logs)
# NOTE: requires AWS CLI configured; we do not pass secrets inline

dev-milkshake-aws-setup vpc_id region log_group role="FlowLogsToCloudWatch":
    @echo "[milkshake:aws] setting up VPC Flow Logs: vpc={{vpc_id}} log_group={{log_group}} role={{role}} region={{region}}"
    bash scripts/cloud_milkshake/aws_flowlogs_setup.sh {{vpc_id}} {{log_group}} {{role}} {{region}}

# Example filter by ENI + UDP port
# protocol: 17=UDP, 6=TCP

dev-milkshake-aws-query log_group interface_id dstport protocol="17" minutes="5" region="":
    @echo "[milkshake:aws] querying flow logs: lg={{log_group}} eni={{interface_id}} port={{dstport}} proto={{protocol}} region={{region}}"
    bash scripts/cloud_milkshake/run_flowlogs_query.sh "{{log_group}}" "{{interface_id}}" "{{dstport}}" "{{protocol}}" "{{minutes}}" "{{region}}"

# Milkshake (firewall compute) helpers
# Requires sudo for nft (we set up counters only)

dev-milkshake port="51999" seconds="1" pps="5000" payload="":
    @echo "[milkshake] setting up nftables counter on UDP dport={{port}}"
    bash scripts/milkshake/nft_setup.sh {{port}}
    bash scripts/milkshake/nft_reset.sh {{port}}
    @echo "[milkshake] sending UDP probes: host=127.0.0.1 port={{port}} pps={{pps}} seconds={{seconds}}"
    python3 scripts/milkshake/udp_probe.py --host 127.0.0.1 --port {{port}} --pps {{pps}} --seconds {{seconds}} --payload "{{payload}}"
    @echo "[milkshake] reading counters"
    bash scripts/milkshake/nft_read.sh {{port}} | awk '{ print "packets="$1" bytes="$2 }'
