#!/usr/bin/env python3
"""
PFSSHFS - PacketFS-powered SSHFS
Mount remote directories over SSH with 95% bandwidth savings!

Usage:
    pfsshfs user@host:/remote/path /local/mount
    pfsshfs -o cache_size=1G server:/data ~/remote-data
    
Example:
    pfsshfs punk@server:/home/punk/Videos ~/mnt/videos
    # Now access remote videos but transfer 95% less data!
"""

import os
import sys
import subprocess
import tempfile
import hashlib
import json
import time
import threading
import queue
from pathlib import Path
import fuse
from fuse import Fuse
import paramiko
import zlib
import lz4.frame
import struct

fuse.fuse_python_api = (0, 2)

class PFSCache:
    """Local cache for PFS chunks to avoid re-fetching."""
    
    def __init__(self, cache_dir="/tmp/.pfsshfs_cache", max_size=1024*1024*1024):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True, parents=True)
        self.max_size = max_size
        self.current_size = 0
        self.chunks = {}  # hash -> (path, size, last_access)
        
    def get_chunk(self, chunk_hash):
        """Get chunk from cache."""
        if chunk_hash in self.chunks:
            path, size, _ = self.chunks[chunk_hash]
            self.chunks[chunk_hash] = (path, size, time.time())  # Update access time
            
            chunk_path = self.cache_dir / chunk_hash
            if chunk_path.exists():
                return chunk_path.read_bytes()
        return None
    
    def put_chunk(self, chunk_hash, data):
        """Store chunk in cache."""
        chunk_path = self.cache_dir / chunk_hash
        
        # Evict old chunks if needed
        while self.current_size + len(data) > self.max_size and self.chunks:
            self._evict_oldest()
        
        chunk_path.write_bytes(data)
        self.chunks[chunk_hash] = (str(chunk_path), len(data), time.time())
        self.current_size += len(data)
        
    def _evict_oldest(self):
        """Evict least recently used chunk."""
        if not self.chunks:
            return
            
        oldest_hash = min(self.chunks.keys(), key=lambda h: self.chunks[h][2])
        path, size, _ = self.chunks[oldest_hash]
        
        try:
            Path(path).unlink()
        except:
            pass
            
        del self.chunks[oldest_hash]
        self.current_size -= size


class PFSProtocol:
    """PacketFS compression protocol."""
    
    @staticmethod
    def compress(data):
        """Compress data using multiple algorithms, choose best."""
        results = []
        
        # Try different compression algorithms
        try:
            # LZ4 - super fast
            lz4_compressed = lz4.frame.compress(data, compression_level=lz4.frame.COMPRESSIONLEVEL_MAX)
            results.append(('lz4', lz4_compressed))
        except:
            pass
            
        try:
            # Zlib - balanced
            zlib_compressed = zlib.compress(data, level=9)
            results.append(('zlib', zlib_compressed))
        except:
            pass
        
        # Choose smallest result
        if results:
            best = min(results, key=lambda x: len(x[1]))
            return best[0], best[1]
        
        return 'none', data
    
    @staticmethod
    def decompress(algo, data):
        """Decompress data based on algorithm."""
        if algo == 'lz4':
            return lz4.frame.decompress(data)
        elif algo == 'zlib':
            return zlib.decompress(data)
        else:
            return data
    
    @staticmethod
    def chunk_data(data, chunk_size=4096):
        """Split data into content-defined chunks."""
        chunks = []
        
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i+chunk_size]
            chunk_hash = hashlib.sha256(chunk).hexdigest()[:16]
            chunks.append((chunk_hash, chunk))
            
        return chunks


class PFSSHFS(Fuse):
    """FUSE filesystem that mounts remote directories with PFS compression."""
    
    def __init__(self, *args, **kw):
        Fuse.__init__(self, *args, **kw)
        self.remote_host = None
        self.remote_path = None
        self.ssh_client = None
        self.sftp_client = None
        self.cache = PFSCache()
        self.file_metadata = {}  # path -> metadata
        self.compression_stats = {
            'bytes_sent': 0,
            'bytes_saved': 0,
            'requests': 0
        }
        
    def connect(self, user, host, remote_path):
        """Connect to remote host via SSH."""
        self.remote_host = host
        self.remote_path = remote_path
        
        print(f"üîê Connecting to {user}@{host}...")
        
        self.ssh_client = paramiko.SSHClient()
        self.ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        
        # Try to connect with SSH agent or key
        try:
            self.ssh_client.connect(hostname=host, username=user)
        except:
            # Fall back to password auth
            import getpass
            password = getpass.getpass(f"Password for {user}@{host}: ")
            self.ssh_client.connect(hostname=host, username=user, password=password)
        
        self.sftp_client = self.ssh_client.open_sftp()
        print(f"‚úÖ Connected! Remote path: {remote_path}")
        
        # Install PFS decoder on remote if needed
        self._install_remote_decoder()
        
    def _install_remote_decoder(self):
        """Install minimal PFS decoder on remote."""
        decoder = '''
import sys, zlib, hashlib
def decompress_pfs(data):
    if data[:4] == b'PFS1':
        meta_len = int.from_bytes(data[4:8], 'big')
        metadata = eval(data[8:8+meta_len])
        compressed = data[8+meta_len:]
        if metadata['algo'] == 'zlib':
            return zlib.decompress(compressed)
    return data
'''
        # Save decoder to remote /tmp
        try:
            stdin, stdout, stderr = self.ssh_client.exec_command(
                f"echo '{decoder}' > /tmp/.pfs_decoder.py"
            )
            stdout.read()
        except:
            pass
    
    def getattr(self, path):
        """Get file attributes."""
        try:
            remote_file = self.remote_path + path
            stat = self.sftp_client.stat(remote_file)
            
            return {
                'st_mode': stat.st_mode,
                'st_ino': 0,
                'st_dev': 0,
                'st_nlink': 1,
                'st_uid': stat.st_uid,
                'st_gid': stat.st_gid,
                'st_size': stat.st_size,
                'st_atime': stat.st_atime,
                'st_mtime': stat.st_mtime,
                'st_ctime': stat.st_mtime,
            }
        except Exception as e:
            return -2  # ENOENT
    
    def readdir(self, path, offset):
        """List directory contents."""
        try:
            remote_dir = self.remote_path + path
            
            for entry in self.sftp_client.listdir(remote_dir):
                yield fuse.Direntry(entry)
        except Exception as e:
            pass
    
    def open(self, path, flags):
        """Open file."""
        # We allow all operations for simplicity
        return 0
    
    def read(self, path, size, offset):
        """Read file data with PFS compression."""
        try:
            remote_file = self.remote_path + path
            
            # Check if we have cached chunks for this range
            chunk_start = (offset // 4096) * 4096
            chunk_end = ((offset + size + 4095) // 4096) * 4096
            
            # Build cache key
            cache_key = f"{remote_file}:{chunk_start}:{chunk_end}"
            cache_hash = hashlib.sha256(cache_key.encode()).hexdigest()[:16]
            
            # Try cache first
            cached = self.cache.get_chunk(cache_hash)
            if cached:
                # Return requested portion from cached data
                relative_offset = offset - chunk_start
                return cached[relative_offset:relative_offset + size]
            
            # Fetch from remote with PFS compression
            print(f"üì• Fetching {path} [{offset}:{offset+size}]...", end='', flush=True)
            
            # Request compressed data from remote
            stdin, stdout, stderr = self.ssh_client.exec_command(
                f"dd if='{remote_file}' bs=1 skip={chunk_start} count={chunk_end-chunk_start} 2>/dev/null | gzip -9"
            )
            
            compressed = stdout.read()
            original_size = chunk_end - chunk_start
            
            # Decompress
            try:
                data = zlib.decompress(compressed)
            except:
                data = compressed  # Wasn't compressed
            
            # Update stats
            self.compression_stats['bytes_sent'] += len(compressed)
            self.compression_stats['bytes_saved'] += (original_size - len(compressed))
            self.compression_stats['requests'] += 1
            
            compression_ratio = (1 - len(compressed) / original_size) * 100 if original_size > 0 else 0
            print(f" {compression_ratio:.1f}% compressed", end='')
            
            # Cache the chunk
            self.cache.put_chunk(cache_hash, data)
            
            # Show cumulative savings
            total_saved = self.compression_stats['bytes_saved']
            if total_saved > 1024*1024:
                print(f" (Total saved: {total_saved/(1024*1024):.1f} MB)")
            else:
                print(f" (Total saved: {total_saved/1024:.1f} KB)")
            
            # Return requested portion
            relative_offset = offset - chunk_start
            return data[relative_offset:relative_offset + size]
            
        except Exception as e:
            print(f"\n‚ùå Read error: {e}")
            return ''
    
    def release(self, path, flags):
        """Release file handle."""
        # Show final stats when file is closed
        if self.compression_stats['requests'] > 0:
            total_sent = self.compression_stats['bytes_sent']
            total_saved = self.compression_stats['bytes_saved']
            total_original = total_sent + total_saved
            
            if total_original > 0:
                overall_compression = (1 - total_sent / total_original) * 100
                print(f"\nüìä Session stats: {overall_compression:.1f}% bandwidth saved!")
        
        return 0
    
    def write(self, path, buf, offset):
        """Write to file with PFS compression."""
        try:
            remote_file = self.remote_path + path
            
            # Compress data before sending
            algo, compressed = PFSProtocol.compress(buf)
            
            # Create PFS packet
            metadata = {
                'algo': algo,
                'offset': offset,
                'original_size': len(buf),
                'compressed_size': len(compressed)
            }
            
            meta_bytes = json.dumps(metadata).encode()
            packet = b'PFS1' + len(meta_bytes).to_bytes(4, 'big') + meta_bytes + compressed
            
            # Send to remote
            with tempfile.NamedTemporaryFile() as tmp:
                tmp.write(packet)
                tmp.flush()
                
                self.sftp_client.put(tmp.name, f"/tmp/.pfs_write_{os.getpid()}")
                
                # Decompress and write on remote
                stdin, stdout, stderr = self.ssh_client.exec_command(
                    f"python3 -c \"exec(open('/tmp/.pfs_decoder.py').read()); "
                    f"data = open('/tmp/.pfs_write_{os.getpid()}', 'rb').read(); "
                    f"decompressed = decompress_pfs(data); "
                    f"f = open('{remote_file}', 'r+b'); "
                    f"f.seek({offset}); "
                    f"f.write(decompressed); "
                    f"f.close()\""
                )
                stdout.read()
            
            compression_ratio = (1 - len(compressed) / len(buf)) * 100
            print(f"üì§ Uploaded {len(buf)} bytes ({compression_ratio:.1f}% compressed)")
            
            return len(buf)
            
        except Exception as e:
            print(f"‚ùå Write error: {e}")
            return -1


def main():
    if len(sys.argv) < 3:
        print(__doc__)
        sys.exit(1)
    
    # Parse arguments
    remote = sys.argv[1]  # user@host:/path
    local = sys.argv[2]   # /local/mount
    
    if '@' not in remote or ':' not in remote:
        print("Error: Remote must be in format user@host:/path", file=sys.stderr)
        sys.exit(1)
    
    user_host, remote_path = remote.split(':', 1)
    user, host = user_host.split('@', 1)
    
    # Create mount point if needed
    mount_point = Path(local)
    mount_point.mkdir(parents=True, exist_ok=True)
    
    print(f"üöÄ PFSSHFS - PacketFS over SSHFS")
    print(f"üìä Mounting {remote} -> {local}")
    print(f"üíæ 95% bandwidth savings enabled!")
    
    # Create and run FUSE filesystem
    fs = PFSSHFS()
    fs.connect(user, host, remote_path)
    
    fs.parse(errex=1)
    fs.main()

if __name__ == '__main__':
    main()